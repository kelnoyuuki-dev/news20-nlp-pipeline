ğŸ“° News20 NLP Pipeline



A structured Natural Language Processing (NLP) pipeline built on the 20 Newsgroups dataset for:



ğŸ”¹ Part 1 â€” Classic Text Classification (BoW / TF-IDF)



ğŸ”¹ Part 2 â€” SentenceTransformer Embedding Classification



ğŸ”¹ Part 3 â€” KMeans Clustering + Topic Tree Generation



This project demonstrates both supervised classification and unsupervised topic modeling using modern NLP techniques.



ğŸ“š Dataset

20 Newsgroups



~18,000 documents



20 balanced categories



Multi-class classification problem



Categories include:



comp.\*



rec.\*



sci.\*



talk.\*



misc.\*



The dataset is automatically downloaded via sklearn.datasets.fetch\_20newsgroups.



ğŸ—‚ Project Structure

news20-nlp-pipeline/

â”‚

â”œâ”€â”€ scripts/

â”‚   â”œâ”€â”€ run\_part1.py

â”‚   â”œâ”€â”€ run\_part2.py

â”‚   â””â”€â”€ run\_part3.py

â”‚

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ cluster\_utils.py

â”‚   â”œâ”€â”€ cluster\_viz.py

â”‚   â”œâ”€â”€ data\_loader.py

â”‚   â”œâ”€â”€ embedding\_cache.py

â”‚   â”œâ”€â”€ llm\_packets.py

â”‚   â”œâ”€â”€ metrics.py

â”‚   â”œâ”€â”€ part1\_classic.py

â”‚   â”œâ”€â”€ part2\_embeddings.py

â”‚   â””â”€â”€ utils.py

â”‚

â”œâ”€â”€ outputs/

â”‚   â”œâ”€â”€ part1/

â”‚   â”œâ”€â”€ part2/

â”‚   â”œâ”€â”€ part3/

â”‚   â””â”€â”€ cache/

â”‚

â”œâ”€â”€ requirements.txt

â””â”€â”€ README.md





All generated files are saved under the outputs/ directory.



âš™ï¸ Setup

1ï¸âƒ£ Create Virtual Environment

python -m venv .venv





Activate it:



Windows

.venv\\Scripts\\activate



Mac / Linux

source .venv/bin/activate



2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt





If you want UMAP visualizations in Part 3:



pip install umap-learn



ğŸš€ How to Run

ğŸ”¹ Part 1 â€” Classic Text Classification



Uses:



CountVectorizer (BoW)



TF-IDF



Logistic Regression / other ML models



Confusion matrix + evaluation metrics



Example Commands

Run with TF-IDF

python -m scripts.run\_part1 --vectorizer tfidf --save\_confusion\_png



Run with Bag-of-Words

python -m scripts.run\_part1 --vectorizer bow --save\_confusion\_png



Outputs

outputs/part1/

â”œâ”€â”€ confusion\_matrix\_best\_\*.png

â”œâ”€â”€ run\_metadata.json

â””â”€â”€ top\_confusion\_pairs.json



ğŸ”¹ Part 2 â€” SentenceTransformer Classification



Uses:



all-MiniLM-L6-v2 embeddings



Cached embeddings (for faster reruns)



ML classifier



Confusion matrix + metrics



Example Command

python -m scripts.run\_part2 --st\_model all-MiniLM-L6-v2





Optional:



--batch\_size 64

--normalize



Outputs

outputs/part2/

â”œâ”€â”€ confusion\_matrix\_best\_\*.png

â”œâ”€â”€ run\_metadata.json

â””â”€â”€ top\_confusion\_pairs.json





Embeddings are cached in:



outputs/cache/





If cache exists, embeddings are not recomputed.



ğŸ”¹ Part 3 â€” Clustering + Topic Tree



Uses:



SentenceTransformer embeddings



Elbow method (K = 2â€“9)



KMeans clustering



2-level hierarchical clustering



TF-IDF fallback topic labeling



Optional PCA / UMAP visualization



LLM labeling packet generation



Example Commands

Basic run

python -m scripts.run\_part3



With embedding normalization

python -m scripts.run\_part3 --normalize



Force specific number of clusters

python -m scripts.run\_part3 --k\_override 6



Generate PCA cluster plots

python -m scripts.run\_part3 --plot pca



Generate UMAP plots (requires umap-learn)

python -m scripts.run\_part3 --plot umap



What Part 3 Does

Step A â€” Top-Level Clustering



Runs elbow search (K=2..9)



Selects optimal K



Clusters all documents



Step B â€” Second-Level Clustering



Identifies 2 largest clusters



Splits each into exactly 3 subclusters



Generates subtopic labels



Step C â€” Partial Topic Tree



Displays and saves simple hierarchical topic tree



Outputs

outputs/part3/

â”œâ”€â”€ part3\_elbow.png

â”œâ”€â”€ cluster\_scatter\_pca\_top.png

â”œâ”€â”€ cluster\_scatter\_pca\_sub\_parentX.png

â”œâ”€â”€ part3\_top\_clusters.json

â”œâ”€â”€ part3\_subclusters.json

â”œâ”€â”€ topic\_tree.txt

â””â”€â”€ llm\_packets/



â± Runtime Notes



Part 1: Fast (<1 min)



Part 2: ~2â€“5 min (first run downloads model)



Part 3:



Embeddings: 2â€“4 min



Clustering: <1 min



UMAP: +1â€“2 min



Subsequent runs are faster due to embedding caching.



ğŸ“œ License



MIT License â€” Educational Use.



You are free to use, modify, and extend this code for academic and learning purposes.

